{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shiyao/anaconda3/envs/neo/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shiyao/anaconda3/envs/neo/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shiyao/anaconda3/envs/neo/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shiyao/anaconda3/envs/neo/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shiyao/anaconda3/envs/neo/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shiyao/anaconda3/envs/neo/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/shiyao/anaconda3/envs/neo/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shiyao/anaconda3/envs/neo/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shiyao/anaconda3/envs/neo/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shiyao/anaconda3/envs/neo/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shiyao/anaconda3/envs/neo/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shiyao/anaconda3/envs/neo/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing.pool import Pool \n",
    "from synthesizer import audio\n",
    "from functools import partial\n",
    "from itertools import chain\n",
    "# from encoder import inference as encoder\n",
    "from pathlib import Path\n",
    "from utils import logmmse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_aishell(datasets_root, out_dir, n_processes, \n",
    "                           skip_existing, hparams, pairs):\n",
    "    # Gather the input directories\n",
    "    \n",
    "    print(\"\\n  Using data from:  \" + datasets_root)\n",
    "    \n",
    "    # Create the output directories for each output file type\n",
    "    out_dir.joinpath(\"mels\").mkdir(exist_ok=True)\n",
    "    out_dir.joinpath(\"audio\").mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create a metadata file\n",
    "    metadata_fpath = out_dir.joinpath(\"train.txt\")\n",
    "    metadata_file = metadata_fpath.open(\"a\" if skip_existing else \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    #print(speaker_dirs)\n",
    "    func = partial(preprocess_speaker, out_dir=out_dir, skip_existing=skip_existing, \n",
    "                   hparams=hparams, datasets_root=datasets_root)\n",
    "    job = Pool(n_processes).imap(func, pairs)\n",
    "    for speaker_metadata in tqdm(job, \"AI-SHELL\", len(pairs), unit=\"speakers\"):\n",
    "        for metadatum in speaker_metadata:\n",
    "            metadata_file.write(\"|\".join(str(x) for x in metadatum) + \"\\n\")\n",
    "    metadata_file.close()\n",
    "\n",
    "    # Verify the contents of the metadata file\n",
    "    with metadata_fpath.open(\"r\", encoding=\"utf-8\") as metadata_file:\n",
    "        metadata = [line.split(\"|\") for line in metadata_file]\n",
    "    mel_frames = sum([int(m[4]) for m in metadata])\n",
    "    timesteps = sum([int(m[3]) for m in metadata])\n",
    "    sample_rate = hparams.sample_rate\n",
    "    hours = (timesteps / sample_rate) / 3600\n",
    "    print(\"The dataset consists of %d utterances, %d mel frames, %d audio timesteps (%.2f hours).\" %\n",
    "          (len(metadata), mel_frames, timesteps, hours))\n",
    "    print(\"Max input length (text chars): %d\" % max(len(m[5]) for m in metadata))\n",
    "    print(\"Max mel frames length: %d\" % max(int(m[4]) for m in metadata))\n",
    "    print(\"Max audio timesteps length: %d\" % max(int(m[3]) for m in metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: update this acoordingly\n",
    "def get_ssb_audio_relative_path(sid : str) -> str : \n",
    "    \"\"\"\n",
    "    returns relative path to utterance (sentence) according to given `sid`\n",
    "    \"\"\"\n",
    "    spkid = sid[:7]\n",
    "    return os.path.join(spkid, f'{sid}.wav')\n",
    "\n",
    "def preprocess_speaker(speaker_dir, out_dir: Path, skip_existing: bool, hparams, datasets_root):\n",
    "    metadata = []\n",
    "    #print(speaker_dir.glob(\"*.wav\"))\n",
    "    #wav_paths = list(chain.from_iterable(speaker_dir.glob(\"*.wav\")))\n",
    "    wav_texts = list( (os.path.join(datasets_root, get_ssb_audio_relative_path(x[0])), x[2]) for x in speaker_dir)\n",
    "    wavs = []\n",
    "    texts = []\n",
    "    wav_paths = []\n",
    "    for wav_path, text in wav_texts:\n",
    "        wav = split_on_silences(wav_path, hparams)\n",
    "        wavs.append(wav)\n",
    "        texts.append(text)\n",
    "        wav_paths.append(wav_path)\n",
    "    assert len(wav_paths) == len(wavs) == len(texts)\n",
    "    for i, (wav, text) in enumerate(zip(wavs, texts)):\n",
    "            metadata.append(process_utterance(wav, text, out_dir, wav_paths[i].split('/')[-1], \n",
    "                                            skip_existing, hparams))\n",
    "    return [m for m in metadata if m is not None]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_on_silences(wav_fpath, hparams):\n",
    "    # Load the audio waveform\n",
    "    wav, _ = librosa.load(wav_fpath, hparams.sample_rate)\n",
    "    if hparams.rescale:\n",
    "        wav = wav / np.abs(wav).max() * hparams.rescaling_max\n",
    "    \n",
    "    return wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_utterance(wav, text, out_dir, basename, \n",
    "                      skip_existing, hparams):\n",
    "    ## FOR REFERENCE:\n",
    "    # For you not to lose your head if you ever wish to change things here or implement your own\n",
    "    # synthesizer.\n",
    "    # - Both the audios and the mel spectrograms are saved as numpy arrays\n",
    "    # - There is no processing done to the audios that will be saved to disk beyond volume  \n",
    "    #   normalization (in split_on_silences)\n",
    "    # - However, pre-emphasis is applied to the audios before computing the mel spectrogram. This\n",
    "    #   is why we re-apply it on the audio on the side of the vocoder.\n",
    "    # - Librosa pads the waveform before computing the mel spectrogram. Here, the waveform is saved\n",
    "    #   without extra padding. This means that you won't have an exact relation between the length\n",
    "    #   of the wav and of the mel spectrogram. See the vocoder data loader.\n",
    "    \n",
    "    \n",
    "    # Skip existing utterances if needed\n",
    "    mel_fpath = out_dir.joinpath(\"mels\", \"mel-%s.npy\" % basename)\n",
    "    wav_fpath = out_dir.joinpath(\"audio\", \"audio-%s.npy\" % basename)\n",
    "    if skip_existing and mel_fpath.exists() and wav_fpath.exists():\n",
    "        return None\n",
    "    \n",
    "    # Skip utterances that are too short\n",
    "    if len(wav) < hparams.utterance_min_duration * hparams.sample_rate:\n",
    "        return None\n",
    "    \n",
    "    # Compute the mel spectrogram\n",
    "    mel_spectrogram = audio.melspectrogram(wav, hparams).astype(np.float32)\n",
    "    mel_frames = mel_spectrogram.shape[1]\n",
    "    \n",
    "    # Skip utterances that are too long\n",
    "    if mel_frames > hparams.max_mel_frames and hparams.clip_mels_length:\n",
    "        return None\n",
    "    \n",
    "    # Write the spectrogram, embed and audio to disk\n",
    "    np.save(mel_fpath, mel_spectrogram.T, allow_pickle=False)\n",
    "    np.save(wav_fpath, wav, allow_pickle=False)\n",
    "    \n",
    "    # Return a tuple describing this training example\n",
    "    return wav_fpath.name, mel_fpath.name, \"embed-%s.npy\" % basename, len(wav), mel_frames, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START PROCESSING\n",
    "## step one : gather metadata description into bathces\n",
    "please change parameter in the `open` call to an appropraite input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=200  # was 200\n",
    "pairs = []\n",
    "with open('datasets/aishell3/metadata.csv') as f:\n",
    "    subpairs = []\n",
    "    begin = True\n",
    "    for line in f.readlines():\n",
    "        subpairs.append(line.strip().split('|'))\n",
    "        if len(subpairs) % batch_size == 0 and not begin:\n",
    "            pairs.append(subpairs)\n",
    "            subpairs = []\n",
    "            \n",
    "        begin = False\n",
    "    if len(subpairs) > 0:\n",
    "        pairs.append(subpairs)\n",
    "\n",
    "\n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SSB00430444',\n",
       " '持起%红缨枪%追赶%对方%半公里$',\n",
       " 'CH IY2 Q IY3 % HH UH2 NG2 Y IY1 NG1 Q IY1 AE1 NG1 % JH UW1 IY1 G AE3 N3 % D UW4 IY4 F AE1 NG1 % B AE4 N4 G UH1 NG1 L IY3 $',\n",
       " 'SSB0043']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step two : preprocess and write to dataset directory\n",
    "1. specify directory to raw data as `rootpath`.\n",
    "\n",
    "the expected structure of this directory:\n",
    "```\n",
    "<rootpath>/<speaker-id>/<sentence-id>.wav\n",
    "```\n",
    "the `speaker-id` is derived from sentence-ids, you should override `get_ssb_audio_relative_path(sid:str) -> str` accordingly.\n",
    "\n",
    "2. specify output directory as `outpath`, this path is afterwards used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using data from:  /NASdata/AudioData/AISHELL-ASR-SSB/SPEECHDATA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AI-SHELL: 100%|██████████| 324/324 [35:41<00:00,  6.61s/speakers]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset consists of 45123 utterances, 13285093 mel frames, 2652263375 audio timesteps (46.05 hours).\n",
      "Max input length (text chars): 365\n",
      "Max mel frames length: 881\n",
      "Max audio timesteps length: 176161\n"
     ]
    }
   ],
   "source": [
    "from synthesizer.hparams import hparams\n",
    "from pathlib import Path\n",
    "\n",
    "rootpath = '/NASdata/AudioData/AISHELL-ASR-SSB/SPEECHDATA'\n",
    "outpath  = Path('datasets').joinpath('aishell3')\n",
    "outpath.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "preprocess_aishell(\n",
    "    rootpath, \n",
    "    outpath, \n",
    "    20, \n",
    "    True, \n",
    "    hparams, \n",
    "    pairs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
